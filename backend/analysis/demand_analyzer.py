import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import re
from collections import Counter
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

logger = logging.getLogger(__name__)

# ä¸‹è½½NLTKæ•°æ®ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œæ—¶éœ€è¦ï¼‰
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')

class DemandAnalyzer:
    """éœ€æ±‚åˆ†æå¼•æ“ - åˆ†ææå–çš„éœ€æ±‚å¹¶è¯„åˆ†"""
    
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        
        # å…³é”®è¯åº“
        self.tool_keywords = {
            'browser_extension': ['extension', 'chrome', 'firefox', 'browser', 'plugin', 'addon'],
            'api_service': ['api', 'rest', 'graphql', 'endpoint', 'integration'],
            'cli_tool': ['cli', 'command line', 'terminal', 'shell', 'script'],
            'mobile_app': ['app', 'mobile', 'ios', 'android', 'phone'],
            'desktop_app': ['desktop', 'windows', 'mac', 'linux', 'application'],
            'web_app': ['web', 'website', 'saas', 'cloud', 'online'],
            'automation': ['automate', 'automation', 'bot', 'robot', 'schedule'],
            'analytics': ['analytics', 'dashboard', 'metrics', 'report', 'statistics'],
            'monitoring': ['monitor', 'alert', 'notification', 'track', 'watch'],
            'productivity': ['productivity', 'efficiency', 'time', 'save', 'fast']
        }
        
        # ä»˜è´¹ä¿¡å·å…³é”®è¯
        self.payment_keywords = {
            'high': ['pay', 'price', 'cost', 'subscription', 'monthly', 'yearly', 
                    'premium', 'enterprise', 'business', 'professional', 'worth'],
            'medium': ['free', 'trial', 'freemium', 'basic', 'standard', 'affordable'],
            'low': ['open source', 'free', 'gratis', 'no cost', 'cheap']
        }
        
        # æŠ€æœ¯å¤æ‚åº¦å…³é”®è¯
        self.complexity_keywords = {
            'high': ['ai', 'machine learning', 'blockchain', 'real-time', 'scalable',
                    'distributed', 'complex', 'advanced', 'sophisticated'],
            'medium': ['database', 'api', 'integration', 'automation', 'dashboard',
                      'analytics', 'monitoring', 'scheduling'],
            'low': ['simple', 'basic', 'lightweight', 'minimal', 'straightforward']
        }
    
    def analyze_demand(self, raw_demand: Dict) -> Dict:
        """åˆ†æå•ä¸ªéœ€æ±‚"""
        try:
            logger.info(f"Analyzing demand: {raw_demand.get('extracted_text', '')[:50]}...")
            
            # æå–æ–‡æœ¬
            text = raw_demand.get('extracted_text', '').lower()
            
            # åŸºç¡€åˆ†æ
            tool_type = self._classify_tool_type(text)
            payment_potential = self._assess_payment_potential(text)
            complexity = self._assess_complexity(text)
            
            # æå–å…³é”®è¯
            keywords = self._extract_keywords(text)
            
            # è¯„åˆ†
            scores = self._calculate_scores(text, tool_type, payment_potential, complexity)
            
            # æ„å»ºåˆ†æç»“æœ
            analysis = {
                "tool_type": tool_type,
                "payment_potential": payment_potential,
                "technical_complexity": complexity,
                "keywords": keywords,
                "scores": scores,
                "analyzed_at": datetime.utcnow().isoformat(),
                "confidence": raw_demand.get('confidence', 0.5),
                "source_info": {
                    "platform": raw_demand.get('source_post', {}).get('platform', 'unknown'),
                    "post_title": raw_demand.get('source_post', {}).get('title', ''),
                    "post_url": raw_demand.get('source_post', {}).get('url', '')
                }
            }
            
            logger.debug(f"Analysis completed: {analysis}")
            return analysis
            
        except Exception as e:
            logger.error(f"Error analyzing demand: {str(e)}")
            return {
                "error": str(e),
                "analyzed_at": datetime.utcnow().isoformat()
            }
    
    def _classify_tool_type(self, text: str) -> str:
        """åˆ†ç±»å·¥å…·ç±»å‹"""
        scores = {}
        
        for tool_type, keywords in self.tool_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in text:
                    score += 1
            
            if score > 0:
                scores[tool_type] = score
        
        if scores:
            # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
            return max(scores.items(), key=lambda x: x[1])[0]
        
        return "unknown"
    
    def _assess_payment_potential(self, text: str) -> str:
        """è¯„ä¼°ä»˜è´¹æ½œåŠ›"""
        scores = {'high': 0, 'medium': 0, 'low': 0}
        
        for level, keywords in self.payment_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    scores[level] += 1
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„çº§åˆ«
        max_level = max(scores.items(), key=lambda x: x[1])
        
        # å¦‚æœæ‰€æœ‰å¾—åˆ†éƒ½ä¸º0ï¼Œè¿”å›medium
        if max_level[1] == 0:
            return "medium"
        
        return max_level[0]
    
    def _assess_complexity(self, text: str) -> str:
        """è¯„ä¼°æŠ€æœ¯å¤æ‚åº¦"""
        scores = {'high': 0, 'medium': 0, 'low': 0}
        
        for level, keywords in self.complexity_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    scores[level] += 1
        
        max_level = max(scores.items(), key=lambda x: x[1])
        
        if max_level[1] == 0:
            return "medium"
        
        return max_level[0]
    
    def _extract_keywords(self, text: str, top_n: int = 10) -> List[str]:
        """æå–å…³é”®è¯"""
        try:
            # åˆ†è¯
            tokens = word_tokenize(text.lower())
            
            # ç§»é™¤åœç”¨è¯å’Œæ ‡ç‚¹
            filtered_tokens = [
                token for token in tokens 
                if token.isalnum() and token not in self.stop_words
            ]
            
            # ç»Ÿè®¡è¯é¢‘
            word_freq = Counter(filtered_tokens)
            
            # è¿”å›æœ€å¸¸è§çš„è¯
            return [word for word, _ in word_freq.most_common(top_n)]
            
        except Exception as e:
            logger.warning(f"Error extracting keywords: {str(e)}")
            return []
    
    def _calculate_scores(self, text: str, tool_type: str, payment_potential: str, complexity: str) -> Dict:
        """è®¡ç®—å„é¡¹è¯„åˆ†"""
        
        # 1. éœ€æ±‚å¼ºåº¦è¯„åˆ† (0-10)
        # åŸºäºæ–‡æœ¬é•¿åº¦ã€ç‰¹å®šå…³é”®è¯ç­‰
        demand_strength = 5.0  # åŸºç¡€åˆ†
        
        # å¢åŠ å¼ºåº¦çš„å…³é”®è¯
        strong_keywords = ['need', 'want', 'must', 'essential', 'critical', 'urgent']
        for keyword in strong_keywords:
            if keyword in text:
                demand_strength += 0.5
        
        # æ–‡æœ¬é•¿åº¦å½±å“
        word_count = len(text.split())
        if word_count > 50:
            demand_strength += 1.0
        elif word_count < 10:
            demand_strength -= 1.0
        
        demand_strength = max(0, min(10, demand_strength))
        
        # 2. å¸‚åœºè§„æ¨¡è¯„åˆ† (0-10)
        # åŸºäºå·¥å…·ç±»å‹å’Œé€šç”¨æ€§
        market_size = 6.0
        
        # é€šç”¨å·¥å…·ç±»å‹æœ‰æ›´å¤§å¸‚åœº
        broad_market_tools = ['web_app', 'browser_extension', 'mobile_app', 'productivity']
        if tool_type in broad_market_tools:
            market_size += 2.0
        
        # å°ä¼—å·¥å…·ç±»å‹å¸‚åœºè¾ƒå°
        niche_tools = ['cli_tool', 'desktop_app']
        if tool_type in niche_tools:
            market_size -= 1.0
        
        market_size = max(0, min(10, market_size))
        
        # 3. ä»˜è´¹æ„æ„¿è¯„åˆ† (0-10)
        payment_willingness = {
            'high': 8.0,
            'medium': 5.0,
            'low': 2.0
        }.get(payment_potential, 5.0)
        
        # 4. æŠ€æœ¯å¯è¡Œæ€§è¯„åˆ† (0-10)
        technical_feasibility = {
            'low': 9.0,   # ä½å¤æ‚åº¦ = é«˜å¯è¡Œæ€§
            'medium': 6.0,
            'high': 3.0   # é«˜å¤æ‚åº¦ = ä½å¯è¡Œæ€§
        }.get(complexity, 6.0)
        
        # 5. è¢«åŠ¨æ”¶å…¥é€‚é…åº¦è¯„åˆ† (0-10)
        passive_income_fit = 5.0
        
        # é€‚åˆè¢«åŠ¨æ”¶å…¥çš„ç‰¹æ€§
        passive_friendly_keywords = ['subscription', 'saas', 'cloud', 'automation', 'api']
        for keyword in passive_friendly_keywords:
            if keyword in text:
                passive_income_fit += 1.0
        
        # å·¥å…·ç±»å‹å½±å“
        passive_friendly_tools = ['web_app', 'api_service', 'automation', 'analytics']
        if tool_type in passive_friendly_tools:
            passive_income_fit += 2.0
        
        passive_income_fit = max(0, min(10, passive_income_fit))
        
        # 6. ç»¼åˆè¯„åˆ† (åŠ æƒå¹³å‡)
        weights = {
            'demand_strength': 0.25,
            'market_size': 0.20,
            'payment_willingness': 0.25,
            'technical_feasibility': 0.15,
            'passive_income_fit': 0.15
        }
        
        overall_score = (
            demand_strength * weights['demand_strength'] +
            market_size * weights['market_size'] +
            payment_willingness * weights['payment_willingness'] +
            technical_feasibility * weights['technical_feasibility'] +
            passive_income_fit * weights['passive_income_fit']
        )
        
        return {
            "demand_strength": round(demand_strength, 1),
            "market_size": round(market_size, 1),
            "payment_willingness": round(payment_willingness, 1),
            "technical_feasibility": round(technical_feasibility, 1),
            "passive_income_fit": round(passive_income_fit, 1),
            "overall": round(overall_score, 1)
        }
    
    def generate_recommendations(self, analysis: Dict) -> Dict:
        """ç”Ÿæˆæ¨èä¿¡æ¯"""
        scores = analysis.get('scores', {})
        tool_type = analysis.get('tool_type', 'unknown')
        
        # æ¨èå®šä»·
        payment_potential = analysis.get('payment_potential', 'medium')
        base_price = {
            'high': 29.99,
            'medium': 14.99,
            'low': 4.99
        }.get(payment_potential, 14.99)
        
        # æ ¹æ®è¯„åˆ†è°ƒæ•´
        overall_score = scores.get('overall', 5.0)
        price_multiplier = overall_score / 10.0 * 1.5  # 0.75-1.5å€
        
        recommended_price = round(base_price * price_multiplier, 2)
        
        # MVPåŠŸèƒ½å»ºè®®
        mvp_features = self._suggest_mvp_features(tool_type)
        
        # æŠ€æœ¯æ ˆå»ºè®®
        tech_stack = self._suggest_tech_stack(tool_type, analysis.get('technical_complexity', 'medium'))
        
        return {
            "recommended_pricing": f"${recommended_price}/month",
            "mvp_features": mvp_features,
            "suggested_tech_stack": tech_stack,
            "time_estimate_weeks": self._estimate_dev_time(analysis.get('technical_complexity', 'medium')),
            "priority": "high" if overall_score >= 7.0 else "medium" if overall_score >= 5.0 else "low"
        }
    
    def _suggest_mvp_features(self, tool_type: str) -> List[str]:
        """æ ¹æ®å·¥å…·ç±»å‹å»ºè®®MVPåŠŸèƒ½"""
        feature_templates = {
            'browser_extension': [
                "Basic content injection/modification",
                "Simple popup interface",
                "Local storage for user preferences",
                "Content script for target websites"
            ],
            'api_service': [
                "RESTful API endpoints",
                "Authentication (API keys)",
                "Rate limiting",
                "Basic documentation"
            ],
            'web_app': [
                "User authentication",
                "Core functionality dashboard",
                "Basic settings page",
                "Responsive design"
            ],
            'automation': [
                "Schedule tasks",
                "Basic error handling",
                "Notification system",
                "Task history/logging"
            ]
        }
        
        return feature_templates.get(tool_type, [
            "Core functionality",
            "User authentication",
            "Basic UI/UX",
            "Error handling"
        ])
    
    def _suggest_tech_stack(self, tool_type: str, complexity: str) -> List[str]:
        """å»ºè®®æŠ€æœ¯æ ˆ"""
        stacks = {
            'browser_extension': ["JavaScript", "HTML/CSS", "Chrome Extension API"],
            'api_service': ["Python/FastAPI", "PostgreSQL", "Docker"],
            'web_app': ["React/Next.js", "Node.js", "PostgreSQL", "Tailwind CSS"],
            'cli_tool': ["Python", "Click library", "Docker"],
            'mobile_app': ["React Native", "Firebase", "Expo"]
        }
        
        base_stack = stacks.get(tool_type, ["Python", "React", "PostgreSQL"])
        
        # æ ¹æ®å¤æ‚åº¦æ·»åŠ æŠ€æœ¯
        if complexity == 'high':
            base_stack.extend(["Docker", "Redis", "Celery", "Monitoring"])
        elif complexity == 'medium':
            base_stack.extend(["Docker", "Basic logging"])
        
        return base_stack
    
    def _estimate_dev_time(self, complexity: str) -> int:
        """ä¼°ç®—å¼€å‘æ—¶é—´ï¼ˆå‘¨ï¼‰"""
        return {
            'low': 2,
            'medium': 4,
            'high': 8
        }.get(complexity, 4)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = DemandAnalyzer()
    
    # æµ‹è¯•éœ€æ±‚
    test_demand = {
        "extracted_text": "I need a tool to automatically sync Google Sheets data to Notion. Currently doing it manually and it takes hours every week. Would pay $20/month for a reliable solution.",
        "confidence": 0.8,
        "source_post": {
            "platform": "hackernews",
            "title": "Looking for Google Sheets to Notion sync tool",
            "url": "https://news.ycombinator.com/item?id=123456"
        }
    }
    
    # åˆ†æéœ€æ±‚
    analysis = analyzer.analyze_demand(test_demand)
    
    print("ğŸ” Demand Analysis Results:")
    print(f"Tool Type: {analysis.get('tool_type')}")
    print(f"Payment Potential: {analysis.get('payment_potential')}")
    print(f"Technical Complexity: {analysis.get('technical_complexity')}")
    print(f"Keywords: {', '.join(analysis.get('keywords', [])[:5])}")
    
    print("\nğŸ“Š Scores:")
    scores = analysis.get('scores', {})
    for key, value in scores.items():
        print(f"  {key}: {value}/10")
    
    # ç”Ÿæˆæ¨è
    recommendations = analyzer.generate_recommendations(analysis)
    
    print("\nğŸ’¡ Recommendations:")
    print(f"Pricing: {recommendations.get('recommended_pricing')}")
    print(f"Dev Time: {recommendations.get('time_estimate_weeks')} weeks")
    print(f"Priority: {recommendations.get('priority')}")
    print(f"Tech Stack: {', '.join(recommendations.get('suggested_tech_stack', []))}")
    
    print(f"\nMVP Features:")
    for i, feature in enumerate(recommendations.get('mvp_features', []), 1):
        print(f"  {i}. {feature}")