import requests
from bs4 import BeautifulSoup
import logging
from datetime import datetime
from typing import List, Dict, Optional
import time
import re

logger = logging.getLogger(__name__)

class HackerNewsCrawler:
    """Hacker Newsçˆ¬è™« - æŠ“å–æŠ€æœ¯å·¥å…·éœ€æ±‚"""
    
    BASE_URL = "https://news.ycombinator.com"
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        })
    
    def fetch_show_hn(self, limit: int = 30) -> List[Dict]:
        """æŠ“å–Show HNå¸–å­ï¼ˆæ–°äº§å“å±•ç¤ºï¼‰"""
        try:
            logger.info(f"Fetching Show HN posts (limit: {limit})")
            
            url = f"{self.BASE_URL}/show"
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è§£æå¸–å­
            posts = []
            rows = soup.select('tr.athing')
            
            for i, row in enumerate(rows[:limit]):
                try:
                    post = self._parse_post(row)
                    if post:
                        posts.append(post)
                        logger.debug(f"Parsed post: {post['title'][:50]}...")
                except Exception as e:
                    logger.warning(f"Failed to parse post {i}: {str(e)}")
                    continue
            
            logger.info(f"Successfully fetched {len(posts)} Show HN posts")
            return posts
            
        except Exception as e:
            logger.error(f"Error fetching Show HN: {str(e)}")
            return []
    
    def fetch_ask_hn(self, limit: int = 30) -> List[Dict]:
        """æŠ“å–Ask HNå¸–å­ï¼ˆé—®é¢˜è®¨è®ºï¼‰"""
        try:
            logger.info(f"Fetching Ask HN posts (limit: {limit})")
            
            url = f"{self.BASE_URL}/ask"
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            posts = []
            rows = soup.select('tr.athing')
            
            for i, row in enumerate(rows[:limit]):
                try:
                    post = self._parse_post(row)
                    if post:
                        posts.append(post)
                except Exception as e:
                    logger.warning(f"Failed to parse Ask HN post {i}: {str(e)}")
                    continue
            
            logger.info(f"Successfully fetched {len(posts)} Ask HN posts")
            return posts
            
        except Exception as e:
            logger.error(f"Error fetching Ask HN: {str(e)}")
            return []
    
    def _parse_post(self, row) -> Optional[Dict]:
        """è§£æå•ä¸ªå¸–å­"""
        try:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = row.select_one('a.titlelink')
            if not title_elem:
                return None
            
            title = title_elem.text.strip()
            url = title_elem.get('href', '')
            
            # å¤„ç†ç›¸å¯¹é“¾æ¥
            if url.startswith('item?'):
                url = f"{self.BASE_URL}/{url}"
            
            # æå–åˆ†æ•°å’Œè¯„è®ºæ•°
            subtext_row = row.find_next_sibling('tr')
            if not subtext_row:
                return None
            
            subtext = subtext_row.select_one('.subtext')
            if not subtext:
                return None
            
            # æå–åˆ†æ•°
            score_elem = subtext.select_one('.score')
            score = 0
            if score_elem:
                score_text = score_elem.text.strip()
                match = re.search(r'(\d+)', score_text)
                if match:
                    score = int(match.group(1))
            
            # æå–è¯„è®ºæ•°
            comments_elem = subtext.find_all('a')[-1]
            comments = 0
            if comments_elem and 'comment' in comments_elem.text:
                comments_text = comments_elem.text.strip()
                match = re.search(r'(\d+)', comments_text)
                if match:
                    comments = int(match.group(1))
            
            # æå–ç”¨æˆ·å’Œæ—¶é—´
            user_elem = subtext.select_one('.hnuser')
            user = user_elem.text.strip() if user_elem else "anonymous"
            
            time_elem = subtext.select_one('.age')
            posted_time = time_elem.text.strip() if time_elem else ""
            
            # æ„å»ºå¸–å­æ•°æ®
            post_data = {
                "title": title,
                "url": url,
                "score": score,
                "comments": comments,
                "user": user,
                "posted_time": posted_time,
                "platform": "hackernews",
                "crawled_at": datetime.utcnow().isoformat(),
                "type": self._classify_post(title)
            }
            
            return post_data
            
        except Exception as e:
            logger.warning(f"Error parsing post: {str(e)}")
            return None
    
    def _classify_post(self, title: str) -> str:
        """æ ¹æ®æ ‡é¢˜åˆ†ç±»å¸–å­ç±»å‹"""
        title_lower = title.lower()
        
        # å·¥å…·ç›¸å…³å…³é”®è¯
        tool_keywords = [
            'tool', 'app', 'website', 'platform', 'service', 'api',
            'library', 'framework', 'cli', 'extension', 'plugin',
            'dashboard', 'analytics', 'monitor', 'automation'
        ]
        
        # é—®é¢˜ç›¸å…³å…³é”®è¯
        problem_keywords = [
            'how to', 'why', 'what', 'which', 'help', 'advice',
            'recommend', 'suggest', 'looking for', 'need', 'want',
            'problem', 'issue', 'challenge', 'pain', 'annoying'
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·å…³é”®è¯
        if any(keyword in title_lower for keyword in tool_keywords):
            return "tool_announcement"
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜å…³é”®è¯
        if any(keyword in title_lower for keyword in problem_keywords):
            return "problem_discussion"
        
        return "other"
    
    def extract_demands_from_post(self, post: Dict) -> List[Dict]:
        """ä»å¸–å­ä¸­æå–æ½œåœ¨éœ€æ±‚"""
        demands = []
        
        try:
            title = post.get('title', '').lower()
            
            # è¯†åˆ«æ½œåœ¨éœ€æ±‚æ¨¡å¼
            demand_patterns = [
                # "I built X to solve Y"
                (r'(built|created|made)\s+(?:a\s+)?(.+?)\s+(?:to|for)\s+(?:solve|fix|help|automate)\s+(.+)', 'tool_solution'),
                # "Looking for a tool that does X"
                (r'looking for (?:a\s+)?(.+?)\s+(?:that|which)\s+(.+)', 'tool_request'),
                # "Is there a tool for X?"
                (r'is there (?:a\s+)?(.+?)\s+for\s+(.+)', 'tool_inquiry'),
                # "How do you handle X?"
                (r'how do you (?:handle|manage|deal with|solve)\s+(.+)', 'problem_question'),
                # "The problem with X is Y"
                (r'the problem with (.+?)\s+is\s+(.+)', 'problem_statement'),
            ]
            
            for pattern, demand_type in demand_patterns:
                match = re.search(pattern, title, re.IGNORECASE)
                if match:
                    demand = {
                        "source_post": post,
                        "demand_type": demand_type,
                        "extracted_text": title,
                        "confidence": 0.7,  # ç½®ä¿¡åº¦è¯„åˆ†
                        "extracted_at": datetime.utcnow().isoformat(),
                        "patterns_found": [pattern]
                    }
                    demands.append(demand)
                    logger.debug(f"Found demand pattern: {demand_type} in '{title[:50]}...'")
            
            # å¦‚æœå¸–å­æœ‰å¾ˆå¤šè¯„è®ºï¼Œå¯èƒ½åŒ…å«æ›´å¤šéœ€æ±‚è®¨è®º
            if post.get('comments', 0) > 10:
                # è¿™é‡Œå¯ä»¥æ·»åŠ æŠ“å–è¯„è®ºçš„é€»è¾‘
                pass
            
            return demands
            
        except Exception as e:
            logger.error(f"Error extracting demands from post: {str(e)}")
            return []
    
    def crawl(self, max_posts: int = 50) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹"""
        logger.info(f"Starting HackerNews crawl (max_posts: {max_posts})")
        
        start_time = time.time()
        
        try:
            # æŠ“å–æ•°æ®
            show_hn_posts = self.fetch_show_hn(limit=max_posts//2)
            ask_hn_posts = self.fetch_ask_hn(limit=max_posts//2)
            
            all_posts = show_hn_posts + ask_hn_posts
            
            # æå–éœ€æ±‚
            all_demands = []
            for post in all_posts:
                demands = self.extract_demands_from_post(post)
                all_demands.extend(demands)
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                "total_posts": len(all_posts),
                "show_hn_posts": len(show_hn_posts),
                "ask_hn_posts": len(ask_hn_posts),
                "total_demands_found": len(all_demands),
                "crawl_duration_seconds": time.time() - start_time,
                "crawled_at": datetime.utcnow().isoformat(),
                "platform": "hackernews"
            }
            
            logger.info(f"Crawl completed: {stats}")
            
            return {
                "posts": all_posts,
                "demands": all_demands,
                "stats": stats
            }
            
        except Exception as e:
            logger.error(f"Crawl failed: {str(e)}")
            return {
                "posts": [],
                "demands": [],
                "stats": {
                    "error": str(e),
                    "crawl_duration_seconds": time.time() - start_time,
                    "crawled_at": datetime.utcnow().isoformat()
                }
            }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import json
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = HackerNewsCrawler()
    
    # æ‰§è¡Œçˆ¬å–
    result = crawler.crawl(max_posts=20)
    
    # æ‰“å°ç»“æœ
    print(f"ğŸ“Š Crawl Stats:")
    print(json.dumps(result["stats"], indent=2))
    
    print(f"\nğŸ“ Found {len(result['demands'])} potential demands:")
    for i, demand in enumerate(result["demands"][:5], 1):
        print(f"{i}. Type: {demand['demand_type']}")
        print(f"   Text: {demand['extracted_text'][:100]}...")
        print(f"   Confidence: {demand['confidence']}")
        print()
    
    if result["demands"]:
        print(f"âœ… Successfully extracted {len(result['demands'])} potential tool demands from HackerNews!")
    else:
        print("â„¹ï¸ No tool demands found in this crawl.")